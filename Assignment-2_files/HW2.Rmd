---
title: "HW2"
author: "Lizhao"
date: "2022/3/6"
output: md_document
---
Author: 

Jyun_Yu_Cheng

Li_Zhao_Du

Yi_Ji_Gao


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Question 1: 


```{r Q1 lib, include=FALSE}
library(tidyverse)
library(dplyr)
capmetro_UT <- read.csv('https://raw.githubusercontent.com/ChildhoodMoments/ECO395M-1/master/data/capmetro_UT.csv')

```


## plot_1: 
One panel of line graphs that plots average boardings grouped by hour of the day, day of week, and month. You should facet by day of week. 

```{r Q1 plot_1, include=FALSE}
capmetro_UT = mutate(capmetro_UT,
                     day_of_week = factor(day_of_week,
                                          levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
                     month = factor(month,
                                    levels=c("Sep", "Oct","Nov")))

ave_board_hour = capmetro_UT %>%
  group_by(hour_of_day, day_of_week,month) %>%
  summarise(mean_boarding = mean(boarding)) %>%
  ggplot(aes(x = hour_of_day, y = mean_boarding,color = month, ))+
  geom_line()+
  facet_wrap(~day_of_week)+
  labs(title = "average boarding numbers ")

```



```{r average boarding numbers}
plot(ave_board_hour)

```

We can get result from the plot "average boarding numbers" that:

The hour of peak boardings broadly similar across the weekday, increases from the begining unil afternoon, reach at its peak at around 16:00 pm, then it decrease gradually.

But the circumstance is different during weekends, they both keep at a lower level thoughout the weekends. 

According to the red line in the first graph, we can see that the average boarding number of Sep's Monday is smaller than other month in same day(Monday) and its peak is also relative lower than other weekdays, lower than 125, comparing to other months or weekdays. What's more, the red line is always below other two lines in the Monday.

We can also see that the average boarding number of Nov's Friday and Thursday is smaller than other month in the same day(Friday and Thursday and Wednesday). According to the blue line,their peak is only relative around 100.  Also we can see that the blue line is always below other two lines in Thursday and Friday and Wednesday






## plot_2

```{r Q1 plot_2, include=FALSE}
ave_board_temp = capmetro_UT %>%
  group_by(timestamp)%>%
  ggplot(aes(x = temperature, y = boarding, color = weekend)) + 
  geom_point(size = 0.7, alph = 0.5)+
  facet_wrap(~hour_of_day)+
  labs(title = 'boardings (y) vs. temperature (x) in each 15-minute window')
  

```

```{r boardings (y) vs. temperature (x) in each 15-minute window, echo=FALSE}
plot(ave_board_temp)
```

When we hold hour of day and weekend status constant, does temperature seem to have a noticeable effect on the number of UT students riding the bus?

from the graph we can see than the temperature does not affect the boarding numbers significant, for same hour and days, there is not obvious trend that as the temperature increases in a relative high interval(70 ----100), the average boarding number decreases significantly, and also there is not obvious decrease trend during the relative low temperature level (0 --- 50)

we can some times see that during peak time intervals, the highest point is lower as temperature rises


### Q2
```{r Q2 lib dataset, include=FALSE}
library(tidyverse)
library(lubridate)
library(dplyr)
library(rsample)  # for creating train/test splits
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(knitr)
library(mosaic)
#SaratogaHouses = data("SaratogaHouses")
data(SaratogaHouses)

```

## build better model

```{r split dataset, include=FALSE}
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

```

we try three different models, and calculate the RMSE

```{r three different models, include=FALSE}
lm1 = lm(price ~ lotSize + lotSize:age + age + 
           landValue + bathrooms + sewer + centralAir, data=saratoga_train)

lm2 = lm(price ~ lotSize + age + log(landValue) + log(livingArea) + 
           bedrooms + bathrooms +  bedrooms:bathrooms + 
           rooms + centralAir, data=saratoga_train)

lm3 = lm(price ~  lotSize + age + log(landValue) 
         + log(livingArea) + log(landValue):log(livingArea) 
         + bedrooms + bathrooms + rooms + centralAir 
         + fireplaces:waterfront, data=saratoga_train)

```

model_1 : lm(price ~ lotSize + lotSize:age + age + 
           landValue + bathrooms + sewer + centralAir, data=saratoga_train)
           
model_2 : lm(price ~ lotSize + age + log(landValue) + log(livingArea) + 
           bedrooms + bathrooms +  bedrooms:bathrooms + 
           rooms + centralAir, data=saratoga_train)
           
model_3 : lm(price ~ lotSize + age + log(landValue) + log(livingArea) log(landValue):log(livingArea) + bedrooms + bathrooms + rooms + centralAir +      fireplaces:waterfront, data = saratoga_train)



and we can get their out of sample's RMSE like:
```{r linear models RMSE}
rmse(lm1, saratoga_test)
rmse(lm2, saratoga_test)
rmse(lm3, saratoga_test)  

```
so I think model_3 is best model I can get from linear model
build the best K-nearest-neighbor regression model for price
I also use the same variables I used in model_3

```{r knn model, include=FALSE}
k_folds = 5
SaratogaHouses_folds = crossv_kfold(SaratogaHouses, k=k_folds)

k_grid = c(25:125)

cv_SaratogaHouses = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(SaratogaHouses_folds$train, ~ knnreg(log(price) ~ log(landValue) + log(livingArea) + log(landValue):log(livingArea) + bedrooms + bathrooms + rooms + centralAir + fireplaces:waterfront, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, SaratogaHouses_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(k_folds))
} %>% as.data.frame



```

we can get the best k as:
```{r find the best k, include=FALSE}
k_min_rmse = cv_SaratogaHouses %>%
  slice_min(err) %>%
  pull(k)

```

```{r show the best k}
k_min_rmse
```

then averaging the estimate of out-of-sample RMSE over many different random train/test splits, either randomly or by cross-validation.
then we do this 20 times to get the average RMSE based on model_3 coefficient setting

First, we do the linear regression for model_1, model_2, model_3 for 20 times, make sure model_3 has the smallest RMSE



```{r repeat the regression many times, include=FALSE}
library(parallel)
rmse_sim = do(20)*{
  # fresh train/test split
  sara_split =  initial_split(SaratogaHouses, prop=0.8)
  sara_train = training(sara_split)
  sara_test  = testing(sara_split)
  
  # refit our models to this particular split
  # we're using "update" here to avoid having to type out the giant model formulas
  lm1 = update(lm1, data=sara_train)
  lm2 = update(lm2, data=sara_train)
  lm3 = update(lm3, data=sara_train)
  
  # collect the model errors in a single vector
  model_errors = c(rmse(lm1, sara_test), rmse(lm2, sara_test), rmse(lm3, sara_test))
  
  # return the model errors
  model_errors
}
```

we get the average RMSE for each model as:
```{r average RMSE after 20 times for linear model}
colMeans(rmse_sim)

```

then we use coefficient of model_3 get its average RMSE after 20 times

```{r 20 times knn regression, include=FALSE}
rmse_sim_2 = do(20)*{
  # fresh train/test split
  sara_split =  initial_split(SaratogaHouses, prop=0.8)
  sara_train = training(sara_split)
  sara_test  = testing(sara_split)
  
  k_folds = 5
  
  SaratogaHouses_folds = crossv_kfold(SaratogaHouses, k=k_folds)
  k_grid = c(25:125)
  cv_SaratogaHouses = foreach(k = k_grid, .combine='rbind') %dopar% {
    models = map(SaratogaHouses_folds$train, ~ knnreg(scale(price) ~ scale(log(landValue)) + scale(log(livingArea)) + log(landValue):log(livingArea) + scale(bedrooms) + scale(bathrooms) + scale(rooms) + centralAir + fireplaces:waterfront, k=k, data = ., use.all=FALSE))
    errs = map2_dbl(models, SaratogaHouses_folds$test, modelr::rmse)
    c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(k_folds))
  } %>% as.data.frame
  
  k_min_rmse = cv_SaratogaHouses %>%
    slice_min(err) %>%
    pull(k)
  knn_SaratogaHouses_predict = knnreg(scale(price) ~ scale(log(landValue)) + scale(log(livingArea)) + log(landValue):log(livingArea) + scale(bedrooms) + scale(bathrooms) + scale(rooms) + centralAir + fireplaces:waterfront, data=sara_train, k=k_min_rmse)
  
  
  # refit our models to this particular split
  # we're using "update" here to avoid having to type out the giant model formulas
  
  # collect the model errors in a single vector
  
  model_errors = c(rmse(knn_SaratogaHouses_predict, sara_test))
  # return the model errors
  model_errors
}

```

and we can get the average knn model RMSE for twenty times as:

```{r average RMSE after 20 times for knn model }
colMeans(rmse_sim_2)

```

from the model we can see that the model_3 has lower RMSE, which means we can use it to estimate the price. since we will use different results, but 







### Question 3 Classification and retrospective sampling

first we input dataset and make a bar plot of default probability by credit history,
Make a bar plot of default probability by credit history



```{r Q3 , include=FALSE}
library(tidyverse)
library(dplyr)
german_credit <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/german_credit.csv')

table_default_history <-  as.data.frame(table(german_credit$Default, german_credit$history))

prob_default <- german_credit%>%group_by(Default, history) %>% summarise(n = n())%>% 
  group_by(history) %>%mutate(feq = n/sum(n))

barplot_default <- ggplot(data = prob_default)+
  geom_bar(mapping = aes(x = history, y = feq, fill = factor(Default)), position = 'dodge',stat='identity')+
  #must add stat = 'identity' basically telling ggplot2 you will provide the y-values for the barplot, 
  #rather than counting the aggregate number of rows for each x value, which is the default stat=count
  #https://stackoverflow.com/questions/61068031/error-stat-count-can-only-have-an-x-or-y-aesthetic
  labs(title = "probability of default based on their own history ")+
  labs(x = 'history', y = 'probability')

```

```{r Q3 bar_plot, include=FALSE}
plot(barplot_default)

```

then build a logistic regression model for predicting default probability

```{r Q3 build a logistic regression, include=FALSE}

logit_history = glm(Default~duration + amount + installment + age + history + purpose + foreign, data = german_credit, family = 'binomial')

```


```{r Q3 regression coefficient result,}
summary(logit_history)

```

We can see the result that coefficent of history: poor and terible history have a huge negative effect on Default.Check the statstical significant for these variables, it shows they are statistical significant

I don't think this data set is appropiate for building a predictive model, since bank sampled a set of loans that had defaulted for inclusion in the study.


### question 4

```{r Q4 lib, include=FALSE}
library(tidyverse)
library(ggplot2)
library(modelr)
library(rsample)
library(mosaic)
library(lubridate)
library(ModelMetrics)
hotels_dev <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/hotels_dev.csv')
hotels_val <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/hotels_val.csv')


```



Using only the data in hotels.dev.csv, please compare the out-of-sample performance of the following models:

```{r mutate a variable month, include=FALSE}
hotels_dev = mutate(hotels_dev, Time = ymd(arrival_date), month = month(Time) %>% factor())
hotels_val = mutate(hotels_val, Time = ymd(arrival_date), month = month(Time) %>% factor())

hotels_dev_split = initial_split(hotels_dev, prop = 0.8)
hotels_dev_split_train = training(hotels_dev_split)
hotels_dev_split_test = testing(hotels_dev_split)

```

I use 3 linear model and 3 logit model
```{r model, include=FALSE}

# baseline 1: a small model that uses only the market_segment, adults, customer_type, and is_repeated_guest variables as features.
lm1 = lm(children ~ market_segment + adults + customer_type + is_repeated_guest,  data = hotels_dev_split_train,)
lm2 = lm(children ~ . -arrival_date, data = hotels_dev_split_train,)
# transfer arrival_date  as a time stamp,  in a specific format: Y-M-D    feature-engineer.R

lm3 = lm(children ~ . -arrival_date + market_segment:distribution_channel + month, data = hotels_dev_split_train,)

glm1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest,  data = hotels_dev_split_train, family = 'binomial')
glm2 = glm(children ~ . -arrival_date, data = hotels_dev_split_train, family = 'binomial')
glm3 = glm(children ~ . -arrival_date + market_segment:distribution_channel + month, data = hotels_dev_split_train, family = 'binomial')


```


and we get result: 
```{r model result, include=FALSE}
rmse(lm1, hotels_dev_split_test)
rmse(lm2, hotels_dev_split_test)
rmse(lm3, hotels_dev_split_test)
rmse(glm1, hotels_dev_split_test)
rmse(glm2, hotels_dev_split_test)
rmse(glm3, hotels_dev_split_test)

```


```{r, include=FALSE}
#Model validation: step 1
# spamtoy.R

probhat = predict(lm3, newdata = hotels_val)
probs <- exp(probhat)/(1 + exp(probhat))

probhat_2 <- predict(glm3, newdata = hotels_val)



# or get probability directly use
#pred = predict(glm3, newdata = hotels_val, type = 'response')
#head(pred)

#children_hat =  ifelse(probhat >= 0.5, 1, 0)
#table(children = hotels_val$children, children_hat = children_hat)


library(ROCR)

# ROC curve
#https://www.bilibili.com/video/BV1Y64y1Z7Ht
ROCR = prediction(probhat, hotels_val$children)
perf = performance(ROCR, 'tpr','fpr')

ROCR_2 = prediction(probhat_2, hotels_val$children)
perf_2 = performance(ROCR_2, 'tpr', 'fpr')

x <- unlist(perf@x.values)
y <- unlist(perf@y.values)
plotdata <- data.frame(x,y)
names(plotdata) <- c('x', 'y')

g <- ggplot(plotdata)+ 
  geom_path(aes(x = x, y = y, color = x), size =1)+
  labs(x = 'False positive rate', y = 'True positive rate', title = 'ROC Curves based on linear model')
  
x_2 <- unlist(perf_2@x.values)
y_2 <- unlist(perf_2@y.values)
plotdata_2 <- data.frame(x_2,y_2)
names(plotdata_2) <- c('x_2', 'y_2')

g_2  <- ggplot(plotdata_2)+ 
  geom_path(aes(x = x_2, y = y_2, color = x_2), size =1)+
  labs(x = 'False positive rate', y = 'True positive rate', title = 'ROC Curves based on logit model')


```

```{r}
plot(g)
plot(g_2)
```


```{r, include=FALSE}
# B
hotels_val_split = initial_split(hotels_val, prop = 0.8)
hotels_val_split_train = training(hotels_val_split)
hotels_val_split_test = testing(hotels_val_split)
library(caret)

set.seed(100)
indexs = createFolds(1:dim(hotels_val)[1], k = 20, list = TRUE, returnTrain = FALSE)
err_result = c()
model = glm(children ~ . -arrival_date + market_segment:distribution_channel + month, data = hotels_val, family = 'binomial')
for (i in 1:20){
  slice_data = hotels_val[indexs[[i]],]
  prediction = predict(model,newdata = slice_data,type = "response")
  predict_num = round(mean(prediction)*dim(slice_data)[1])
  ### expected number of bookings with children for that fold.
  actual_num = sum(slice_data$children)
  err_result = rbind(err_result,c(actual_num,predict_num))
} 

```


```{r, include=FALSE}
err_result = data.frame(err_result)
colnames(err_result)=c("actual_num","predict_num") 
err_result$difference = err_result$predict_num-err_result$actual_num

```

```{r}
err_result
```

```{r, include=FALSE}
p0 = ggplot(data=err_result) + 
  geom_histogram(aes(x=difference))

```

```{r}
plot(p0)
```

