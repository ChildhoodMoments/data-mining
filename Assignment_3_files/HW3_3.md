## filter data, create a variable calculate the revenue per square foot

When I browsed the data, I found there are some dummy variables(like
renovated, class\_a), since they are indexes, and there are also some
variables, their values are not randomly distributed, like `empl_gr`. So
I just use linear regression and tree model together, based on RMSE to
choose the best model to fit the estimation. There are two indexes we
focus on finding the best model: RMSE and AIC, we try to find the model
with lower RMSE and AIC.

Also, I splited the initial dataset into the training dataset and
testing dataset.

### LASSO method

![](HW3_3_files/figure-markdown_strict/single%20Lasso%20plots-1.png)

    ##     seg1     seg2     seg3     seg4     seg5     seg6     seg7     seg8 
    ## 34295.85 34011.45 33738.56 33479.11 33232.95 32999.91 32779.73 32572.14 
    ##     seg9    seg10    seg11    seg12    seg13    seg14    seg15    seg16 
    ## 32376.79 32193.33 32021.35 31860.42 31710.09 31569.90 31439.37 31318.01 
    ##    seg17    seg18    seg19    seg20    seg21    seg22    seg23    seg24 
    ## 31205.35 31100.90 31004.19 30914.75 30832.12 30755.88 30685.58 30620.84 
    ##    seg25    seg26    seg27    seg28    seg29    seg30    seg31    seg32 
    ## 30561.26 30506.47 30456.14 30409.91 30334.42 30254.86 30181.47 30113.84 
    ##    seg33    seg34    seg35    seg36    seg37    seg38    seg39    seg40 
    ## 30048.10 29983.93 29924.89 29870.61 29820.73 29774.94 29732.92 29694.39 
    ##    seg41    seg42    seg43    seg44    seg45    seg46    seg47    seg48 
    ## 29659.07 29626.72 29597.09 29569.97 29545.16 29522.47 29501.72 29484.14 
    ##    seg49    seg50    seg51    seg52    seg53    seg54    seg55    seg56 
    ## 29464.08 29445.72 29428.94 29413.61 29399.61 29386.83 29375.16 29364.51 
    ##    seg57    seg58    seg59    seg60    seg61    seg62    seg63    seg64 
    ## 29354.79 29345.92 29337.82 29330.44 29323.70 29317.56 29311.96 29306.85 
    ##    seg65    seg66    seg67    seg68    seg69    seg70    seg71    seg72 
    ## 29303.98 29299.19 29297.80 29291.47 29285.68 29280.40 29275.59 29271.20 
    ##    seg73    seg74    seg75    seg76    seg77    seg78    seg79    seg80 
    ## 29268.19 29263.52 29259.27 29255.39 29251.08 29244.88 29243.03 29239.07 
    ##    seg81    seg82    seg83    seg84    seg85    seg86    seg87    seg88 
    ## 29232.17 29225.84 29218.06 29214.70 29209.71 29205.10 29200.91 29197.08 
    ##    seg89    seg90    seg91    seg92    seg93    seg94    seg95    seg96 
    ## 29193.59 29190.41 29189.44 29186.76 29186.16 29183.69 29181.44 29179.39 
    ##    seg97    seg98    seg99   seg100 
    ## 29177.52 29175.82 29174.31 29172.90

![](HW3_3_files/figure-markdown_strict/single%20Lasso%20plots-2.png)

    ## 21 x 1 sparse Matrix of class "dgCMatrix"
    ##                          seg100
    ## intercept         -1.085502e+01
    ## cluster            3.026588e-04
    ## size               7.227363e-06
    ## empl_gr            .           
    ## stories            .           
    ## age               -1.020202e-02
    ## renovated          .           
    ## class_a            3.398936e+00
    ## class_b            1.660211e+00
    ## LEED               6.382605e-01
    ## Energystar         .           
    ## green_rating       1.190230e+00
    ## net               -1.293758e+00
    ## amenities          1.438857e+00
    ## cd_total_07       -1.617424e-04
    ## hd_total07         3.797872e-04
    ## total_dd_07        .           
    ## Precipitation     -5.997064e-03
    ## Gas_Costs          .           
    ## Electricity_Costs  6.516371e+01
    ## City_Market_Rent   1.005355e+00

    ##    seg100 
    ## 0.1100108

In this simple lasso result, we can see that as lambda become smaller,
AIC increasing. On the other hand, we can see that which variables are
statistical significantly. On the other hand, we find some variables are
not significant in the result( their coefficients are 0), but I don’t
plan to drop them, rather than construct some intersections for these
variables. In the next mannual linear regression, I combine `renovated`
with `age`, `total_dd_07` with `Precipitation`,

Next, we will include these variable and mannual set a linear
regression, we will try to cover some intersections for those
unimportant variable in the simple lasso regression.

    ## fold 1,2,3,4,5,6,7,8,9,10,done.

    ## [1] -2.207176

    ## [1] 15

![](HW3_3_files/figure-markdown_strict/cross%20validated%20lasso-1.png)

### A mannual procedure to construct model.

The function we used is:
`lm(revenue) ~ .- CS_PropertyID - 1 - Rent- leasing_rate + renovated:renovated + total_dd_07:Precipitation`
we get the coefficients of the function as well as its AIC and rmse:

    ##                   cluster                      size                   empl_gr 
    ##              2.122117e-04              6.945934e-06             -1.512420e-02 
    ##                   stories                       age                 renovated 
    ##              1.244896e-02             -1.560873e-02             -1.549602e-01 
    ##                   class_a                   class_b                      LEED 
    ##              3.718730e+00              2.103881e+00              2.251099e+00 
    ##                Energystar              green_rating                       net 
    ##              3.498718e-01              8.884006e-01             -1.795918e+00 
    ##                 amenities               cd_total_07                hd_total07 
    ##              1.212752e+00             -1.497975e-03             -1.104097e-03 
    ##               total_dd_07             Precipitation                 Gas_Costs 
    ##                        NA             -3.319983e-01              3.144114e+01 
    ##         Electricity_Costs          City_Market_Rent total_dd_07:Precipitation 
    ##             -1.144335e+01              1.013001e+00              5.169054e-05

    ## [1] 46993.16

    ## Warning in predict.lm(model, data): prediction from a rank-deficient fit may be
    ## misleading

    ## [1] 11.08006

### tree model, based on CART, random forests, and gradient-boosted trees

since we use tree model, we will cover all variables and don’t contain
intersection. When we were building the model, we split only if that
tree have at least 30 obs in a node, and the split improves the fit by a
factor of 0.00001.

Our tree model’s variables are:
`rpart(revenue ~ . - Rent -  leasing_rate - CS_PropertyID)`

#### CART

First, we use CART:

we can get the tree plot and cross-validated error as:
![](HW3_3_files/figure-markdown_strict/CART%20tree%20and%20CV%20plot-1.png)![](HW3_3_files/figure-markdown_strict/CART%20tree%20and%20CV%20plot-2.png)
Then use function to pick the smallest tree, and get the minimum rmse:

    ## [1] 10.27765

#### random forests

we use random forests to do the estimation, based on same tree model’s
dependent variables, and we get its RMSE:

    ## [1] 8.46903

#### gradient-boosted trees

we use gradient-boosted trees model to do the estimation, based on same
dependent variables, and we get its RMSE: now we average over 100
bootstrap samples, this time use **all candidate variables (mtry=20)**
in each bootstrapped sample

    ## [1] 8.183151

#### model compare

<table>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">RMSE</th>
<th style="text-align: left;">AIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">mannual</td>
<td style="text-align: right;">11.08006</td>
<td style="text-align: left;">NA</td>
</tr>
<tr class="even">
<td style="text-align: left;">CART</td>
<td style="text-align: right;">10.27765</td>
<td style="text-align: left;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">random forests</td>
<td style="text-align: right;">8.46903</td>
<td style="text-align: left;">NA</td>
</tr>
<tr class="even">
<td style="text-align: left;">gradient-boosted trees</td>
<td style="text-align: right;">8.18315</td>
<td style="text-align: left;">NA</td>
</tr>
</tbody>
</table>

so we can see that gradient-boosted trees has relative smaller rmse
(which is almost as same as the random forest model), we will continue
use gradient-boosted trees model to quantify the average change in
rental income per square foot (whether in absolute or percentage terms)
associated with green certification,

    partialPlot(gbm_revenue, greenhouse_test, 'LEED', las=1)

![](HW3_3_files/figure-markdown_strict/patrial%20effect%20of%20leed%20and%20energy%20star-1.png)

    partialPlot(gbm_revenue, greenhouse_test, 'Energystar', las=1)

![](HW3_3_files/figure-markdown_strict/patrial%20effect%20of%20leed%20and%20energy%20star-2.png)

from the plot of partial effects for LEED and Energystar, we can see
that their partial effect is constant and stable. According to their
slopes, we can estimate that when a house has a LEED certification, the
average change in rental income per square foot will increase 2 dollars,
holding all else fixed. On the other hand, when a house has a Energystar
certification, the average change in rental income per square foot will
increase 0.1 dollars, holding all else fixed.
