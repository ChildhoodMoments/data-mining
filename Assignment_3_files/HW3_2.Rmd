---
title: "HW3_2"
author: "Lizhao"
date: "2022/3/28"
output: md_document
---


## Tree modeling: dengue cases
In all tree model, we will use the following equation:
`rpart(total_cases~ season + city + specific_humidity+tdtr_k+precipitation_amt)`
and we will use rmse to judge better performance for each model
### import data

```{r read csv, include=FALSE}
dengue <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/dengue.csv')
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(parallel)
library(ModelMetrics)
library(randomForest)
```

we first split the data set into train and test set, and do cross validation in the train data set, then make predictions in the test data set.


```{r split dataset, echo=FALSE}
dengue_split =  initial_split(dengue, prop=0.8)
dengue_train = training(dengue_split)
dengue_test  = testing(dengue_split)

```
## CART
when we use `rpart` default 10-fold cross-validation, then graph the tree plot and its X-relative error. In this case, I set if it have at least 30 obs in a node,and the split improves the fit by a factor of 0.000002
```{r carts, echo=FALSE}
cart_dengue = rpart(total_cases~ season + city + specific_humidity+tdtr_k+precipitation_amt, data=dengue_train,
                  control = rpart.control(cp = 0.0000000002, minsplit=20))

tree_plot = rpart.plot(cart_dengue, digits=-5, type=4, extra=1)
cv_plot = plotcp(cart_dengue)


```


use the '1SE rule' to pick a tree, create a function and choose the smallest tree, and calculate the RMSE. 

```{r function that choose tree, include=FALSE}
cp_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  cp_opt
}

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

```
so we get the CV error is within 1 std err of the minimum level:
```{r smallest CV error, echo=FALSE}
cp_1se(cart_dengue)

```
prunes the tree at that level, and we get its RMSE as:
```{r smallest tree, echo=FALSE}
dengue_prune = prune_1se(cart_dengue)
cart_rmse = modelr::rmse(dengue_prune, dengue_test)
cart_rmse
```


## random forests
we use the same variable as we used in CART:
```{r random_forests, include=FALSE}
set.seed(100)
dengue_forest = randomForest(total_cases ~ season + city + specific_humidity + tdtr_k + precipitation_amt, data=dengue_train %>% drop_na(), importance = TRUE)
```

we get its RMSE:
```{r RMSE for random Forest, echo=FALSE}
rf_rmse = modelr::rmse(dengue_forest, dengue_test)
rf_rmse
```

## gradient-boosted trees

```{r gradient-boosted, include=FALSE}
library(randomForest)
library(gbm)
set.seed(100)
gbm_dengue = randomForest(total_cases ~ season + city + specific_humidity + tdtr_k + precipitation_amt, data=dengue_train %>% drop_na(), mtry = 5, ntree=100)

gbm_case_hat = predict(gbm_dengue, dengue_test %>% drop_na())
gbm_rmse = mean((gbm_case_hat - dengue_test$total_cases)^2) %>% sqrt
```
we can get its RMSE as 
```{r GBM RMSE, include=TRUE}
gbm_rmse
```

### model compare
```{r table for model summary, include=FALSE}
grade <- data.frame(Model = c("CART","random forests", "gradient-boosted trees"),
                    RMSE = c(cart_rmse, rf_rmse, gbm_rmse)
                    )
                    
summary_table = knitr::kable(grade, 'pipe')
```


so in general, we can see that random forest is better than CART and gradient-boosted trees, and we make partial dependence plots for `specific_humidity`, `tdtr_k`, `precipitation_amt`:

```{r randomforest partial plots, echo=FALSE}
partialPlot(dengue_forest, dengue_test %>% drop_na(), 'specific_humidity', las=1)

partialPlot(dengue_forest, dengue_test %>% drop_na(), 'tdtr_k', las=1)
partialPlot(dengue_forest, dengue_test %>% drop_na(), 'precipitation_amt', las=1)

```




