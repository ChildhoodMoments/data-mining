---
title: "HW1"
author: "Lizhao"
date: "2022/2/9"
output:
  md_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## question 1

```{r, include=FALSE, message = FALSE,warning=FALSE}
library(tidyverse)
library(lubridate)
library(RCurl)
library(dplyr)
library(ggplot2)
library(rsample)  # for creating train/test splits
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(knitr)
ABIA <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/ABIA.csv')

```
calculate the average departure delay time over 2008 and make a plot to show that

```{r the plot of average departure delay time, include=FALSE}

AverageDayDelay <- ABIA %>% 
  select(Year, Month, DayofMonth, ArrDelay, DepDelay, ) %>%
  mutate(departureday = make_datetime(Year, Month, DayofMonth)) 

delay <-  AverageDayDelay %>%
  group_by(departureday) %>%
  summarise(mean_dep_delay = mean(DepDelay, na.rm = TRUE))

average_departure_delay_time_over_2008 <- ggplot(data = delay, aes(x = departureday, y = mean_dep_delay)) +
  geom_line()+
  labs(title = "average departure delay time over 2008 ")+
  labs(x = 'date', y = '(minutes)')
```

```{r plot average departure delay time over 2008}
plot(average_departure_delay_time_over_2008)
```

we can see the result that in most time, the delay time fluctulate between 0 and 20, and around Oct, the delay time is relatively small.



```{r average departure delay time over a day, include=FALSE}
AverageSpecificTimeDelay <- ABIA %>%
  select(Year, Month, DayofMonth, CRSDepTime, ArrDelay, DepDelay) %>%
  mutate(departureday = make_datetime(Year, Month, DayofMonth, CRSDepTime %/% 100, CRSDepTime %% 100)) %>%
  mutate(CRSdeparttime = format(departureday, format = "%H"))
  

delayoneday = AverageSpecificTimeDelay %>%
  group_by(CRSdeparttime) %>%
  summarise(mean_dep_oneday = mean(DepDelay, na.rm = TRUE))



delay_day <- ggplot(delayoneday) +
  geom_line(aes(x = CRSdeparttime, y = mean_dep_oneday, group =1)) +
  labs(title = "average departure delay time over one day(measured by hours) ")+
  labs(x = 'time', y = '(minutes)')


```

```{r delay day}
plot(delay_day)
```

we can find from 6am, the average delay time is increased in general, and due to schedule arrangement, there is no flight delarture between 00 and 06

so we give a suggestion: try to catch earlier flight rather than later flight


```{r delay affected by airline, include=FALSE }
answer1 <- ABIA %>% 
  select(Year, Month, DayofMonth, CRSDepTime, DepDelay, UniqueCarrier) %>%
  mutate(departureday = make_datetime(Year, Month, DayofMonth, CRSDepTime %/% 100, CRSDepTime %% 100)) %>%
  mutate(CRSdeparttime = format(departureday, format = "%H"))

delay_airline = answer1 %>%
  group_by(CRSdeparttime, UniqueCarrier) %>%
  summarise(count = n(),
            mean_dep_delay = mean(DepDelay, na.rm = TRUE))

delay_affected_airline <-  delay_airline %>%
  filter(count > 50)%>%
  ggplot(aes(x = CRSdeparttime, y = mean_dep_delay)) +
  geom_line(group = 1)+
  facet_wrap(~UniqueCarrier)

```

```{r delay time affected by airline}
plot(delay_affected_airline)

```

from that plot we can see that overall, US airline has the minimum delay time, but we lack their data in some months, maybe they doesn't arrange flightline during these months


## question 2
```{r question 2, include=FALSE}
billboard <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/billboard.csv')


```







```{r part A, include=FALSE}
# part_A  Make a table of the top 10 most popular songs since 1958, 
#as measured by the total number of weeks that a song spent on the Billboard Top 100. 
the_top_10_most_popular_songs = billboard %>%
  group_by(performer, song)%>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10, part_A)

```


```{r the top 10 most popular songs since 1958 table }
the_top_10_most_popular_songs
```
from the table we can get top 10 most popular songs since 1958

partB
```{r part B first step, include=FALSE}
# part B Is the "musical diversity" of the Billboard Top 100 changing over time?
#excludes the years 1958 and 2021   useful: https://www.datasciencemadesimple.com/remove-duplicate-rows-r-using-dplyr-distinct-function/#:~:text=Cool%20Text%20Symbol-,Remove%20Duplicate%20rows%20in%20R%20using%20Dplyr%20%E2%80%93%20distinct%20()%20function,variable%20or%20with%20multiple%20variable.

part_b =billboard %>%
  filter(year != 1958 & year != 2021) 

```
we first excludes the years 1958 and 2021
then we can counts the number of times that a given song appears on the Top 100 in a given year

```{r second step, include=FALSE}
part_b_1 <- part_b %>%
  group_by(year,song)%>%
  summarise(count = n())
number_of_times_that_a_given_song <- part_b_1 %>% distinct(song, .keep_all = TRUE)

```

```{r result of second step}
number_of_times_that_a_given_song
```

then we count the number of unique songs that appeared on the Top 100 in each year, irrespective of how many times it had appeared.

```{r partB last step, include=FALSE}
part_b_3 <- number_of_times_that_a_given_song %>%
  group_by(year) %>%
  summarise(count = length(unique(song)))

# make a line graph to show muscial diversity over the years
unique_music <- ggplot(data = part_b_3)+
  geom_line(aes(x = year, y = count, group = 1))

```

```{r show musical diversity over the years}
plot(unique_music)
```


partC "ten-week hit" as a single song that appeared on the Billboard Top 100 for at least ten weeks. 
we first find performer and music satisfy ten-week hit, then filter people who have less than 30 songs
```{r part c satisfied music, include=FALSE}
part_c_1 <- billboard %>% 
  group_by(song, performer)%>%
  summarise(count = n())%>%
  arrange(desc(count)) %>%
  filter(count >= 10)

```

```{r filter unqualified musician, include=FALSE}
part_c_2 <- part_c_1 %>%
  group_by(performer) %>%
  summarise(count = n())%>%
  filter(count >= 30) %>%
  arrange(desc(count))
```

we can get the plot like: 
```{r ten-week hit, include=FALSE}
ten_week_hit_musicians <- ggplot(data = part_c_2, )+
  geom_col(aes(reorder(performer, count), count))+
  coord_flip()
  
```

```{r ten_week_hit_musicians list}
plot(ten_week_hit_musicians)
```

# question4 
```{r, echo =FALSE}

sclass <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/sclass.csv")
```

##1 Filter 350 & 65 AMG
```{r, echo =FALSE}
def_350 = sclass%>%
  filter(trim == "350")

def_65 = sclass%>%
  filter(trim == "65 AMG")
```

##2 spilt to test& training set
```{r, echo =FALSE}
def_350_spilt = initial_split(def_350 ,prop=0.8)
def_350_train = training(def_350_spilt)
def_350_test = testing(def_350_spilt)

def_65_spilt = initial_split(def_65 ,prop=0.8)
def_65_train = training(def_65_spilt)
def_65_test = testing(def_65_spilt)
```

##3 run k nearest neighbors  RMSEs
```{r, echo =FALSE}
k_folds = 5
k_grid = rep(1:100)

knn100 = knnreg(price ~ mileage, data=def_350_train, k=100)
rmse(knn100, def_350_test)

knn100 = knnreg(price ~ mileage, data=def_65_train, k=100)
rmse(knn100, def_65_test)

# Pipeline 1:
# create specific fold IDs for each row
# the default behavior of sample actually gives a permutation
def_350 = def_350 %>%
  mutate(fold_id = rep(1:k_folds, length=nrow(def_350)) %>% sample)

head(def_350)

def_65 = def_65 %>%
  mutate(fold_id = rep(1:k_folds, length=nrow(def_65)) %>% sample)

head(def_65)

def_350_folds = crossv_kfold(def_350, k=k_folds)
def_65_folds = crossv_kfold(def_65, k=k_folds)

cv_grid_350 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(def_350_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, def_350_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(k_folds))
} %>% as.data.frame

cv_grid_65 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(def_65_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, def_65_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(k_folds))
} %>% as.data.frame

head(cv_grid_350)
head(cv_grid_65)

```

##4 每個trim都畫出RMSE和K的關係，可看出optimal k（line or point）
```{r, echo =FALSE}
ggplot(cv_grid_350) + 
  geom_point(aes(x= k, y= err)) + 
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err))

ggplot(cv_grid_65) + 
  geom_point(aes(x= k, y= err)) + 
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err))
```
The optimal k for 350 is 15, and the optimal k for 65 AMG is 22

##5 該k值對應的model(for each trim)
```{r, echo =FALSE}
k_min_rmse_350 = cv_grid_350 %>%
  slice_min(err) %>%
  pull(k)

k_min_rmse_65 = cv_grid_65 %>%
  slice_min(err) %>%
  pull(k)
```

##6哪個trim有較大的optimal k? why?
```{r, echo =FALSE}
# 350 
def_350_split = initial_split(def_350, prop=0.8)
def_350_train = training(def_350_split)
def_350_test  = testing(def_350_split)

knn100 = knnreg(price ~ mileage, data=def_350_train, k=100)
rmse(knn100, def_350_test)

def_350_test = def_350_test %>%
  mutate(price_350_pred = predict(knn100, def_350_test))

p_test_350 = ggplot(data = def_350_test) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) +
  geom_line(aes(x = mileage , y = price_350_pred), color='red', size=1.5)+ ggtitle("350")

p_test_350

#65 AMG
def_65_split = initial_split(def_65, prop=0.8)
def_65_train = training(def_65_split)
def_65_test  = testing(def_65_split)

knn100 = knnreg(price ~ mileage, data=def_65_train, k=100)
rmse(knn100, def_65_test)

def_65_test = def_65_test %>%
  mutate(price_65_pred = predict(knn100, def_65_test))

p_test_65 = ggplot(data = def_65_test) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) +
  geom_line(aes(x = mileage , y = price_65_pred), color='red', size=1.5)+ggtitle(" 65 AMG")

p_test_65
```

65 AMG has a bigger optimal value of k.

Because 65 AMG trim has more data than 350 trim level.




