---
title: "final_project"
author: "Lizhao"
date: '2022-05-04'
output: 
  md_document:
  toc: TRUE
  toc_float: TRUE
---


I collected data on all players of FIFA2019 on the kaggle website [data set](https://raw.githubusercontent.com/ChildhoodMoments/data-mining/main/data.csv), including height, weight, and various ability values. As one of the most popular football games, it introduces many variables to measure the ability of players. In this project, I found that each player has their own ___Preferred Foot___, the number of players with the preferred left foot is less, but the average and median of their multiple ability values are greater than the players with the preferred right foot, I guess this is an official setting of the game. For this binary variable of "predominant foot", I tried to analyze whether it is possible for us to predict the player's preferred foot through various ability values of each player. Of course, this is based on the setting of official FIFA games. It does not mean the same result in reality.


# Abstract 
My question is: Can we predict the preferred foot of a player based on his various abilities? If I could, I could know where the strengths of different footed players are. I used __LASSO__, __Logistics model (based on AIC and CV)__, __stepwise function__, __RandomForest model__ to predict this binary variable (in most cases, each player should have only one dominant foot from left foot or right foot, but due to the missing data of some players, this data set does not show their data, we will filter out these in the data preprocessing), and finally we came to a conclusion based on the ROC curve graph, AUC core and f1 Score. 


# Introduction
I extracted the player's age, height, weight, and various ability values (such as dribbling, crossing, etc.) In this project, I set a binary variable __preferred foot index__, when the player's dominant foot is the right foot, this index is 1, otherwise it is 0.

Let's first look at a data comparison of preferred left foot and preferred right foot players (In this project we analyze the player's  other ability values):


```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(gamlr)
library(rsample)
library(knitr)
library(rpart)
library(randomForest)
library(foreach)
library(ModelMetrics)
library(pROC)
library(tibble)
library(glmnet)

#data <- read.csv('D:/01 UT Austin/2022 spring/big data python/eco395m-HW5/data.csv', encoding = 'UTF-8')
data <- read.csv('https://raw.githubusercontent.com/ChildhoodMoments/data-mining/main/data.csv', encoding = 'UTF-8')


```


```{r preprocessig dataset, include=FALSE}
Analyzed_data <- data %>% 
  na_if("")%>% select("Name","Age","Wage","Overall","Potential","Club","Value", "Position","Height","Weight","Preferred.Foot","Potential","Nationality","Crossing","Finishing","HeadingAccuracy",
                      "ShortPassing","Volleys","Dribbling","Curve",
                      "FKAccuracy","LongPassing","BallControl",
                      "Acceleration","SprintSpeed","Agility",
                      "Reactions","Balance","ShotPower","Jumping",
                      "Stamina","Strength","LongShots","Aggression",
                      "Interceptions","Positioning","Vision",
                      "Penalties","Composure","Marking",
                      "StandingTackle","SlidingTackle","GKDiving",
                      "GKHandling","GKKicking","GKPositioning",
                      "GKReflexes") 

# https://stackoverflow.com/questions/56548774/using-str-extract-to-extract-dollar-amounts
# extract their wage number
Analyzed_data <- Analyzed_data %>%   mutate(wage_amount = parse_number(str_match(Wage,"\\€([0-9,.]+)")[,2]))

# extract their height and transfer into centimeter
#https://stackoverflow.com/questions/54693260/height-conversion-in-r
Analyzed_data <- Analyzed_data %>% separate(Height, c('feet', 'inches'), "'", convert = TRUE)  
Analyzed_data <- Analyzed_data %>%mutate(height_cm = (12*feet + inches)*2.54)

# extract their weight
#Analyzed_data <- Analyzed_data %>% mutate(weight_amount = gsub("[^0-9]","",Analyzed_data$Weight))
# https://stackoverflow.com/questions/14543627/extracting-numbers-from-vectors-of-strings

Analyzed_data <- Analyzed_data %>%   mutate(weight_amount = parse_number(str_match(Weight,"[0-9,.]+lbs")))

# transfer PPreferred.Foot == 'Right' equal to 1, Preferred.Foot == 'Left' equal to 0
Analyzed_data <- Analyzed_data %>% mutate(perfoot_index =  ifelse(Preferred.Foot == 'Right', c(1), c(0)))
Analyzed_data <- na.omit(Analyzed_data)  
```


```{r set up foot prediction, echo=FALSE}

Y <- Analyzed_data %>% 
  na_if("")%>% select("perfoot_index","Age","height_cm","weight_amount","Crossing","Finishing","HeadingAccuracy",
                      "ShortPassing","Volleys","Dribbling","Curve",
                      "FKAccuracy","LongPassing","BallControl",
                      "Acceleration","SprintSpeed","Agility",
                      "Reactions","Balance","ShotPower","Jumping",
                      "Stamina","Strength","LongShots","Aggression",
                      "Interceptions","Positioning","Vision",
                      "Penalties","Composure","Marking",
                      "StandingTackle","SlidingTackle","GKDiving",
                      "GKHandling","GKKicking","GKPositioning",
                      "GKReflexes")

left_foot <- subset(Y, perfoot_index == 0)
right_foot <-  subset(Y, perfoot_index == 1)

left_foot_sum <- summary(left_foot)%>% as.data.frame()
right_foot_sum <- summary(right_foot)%>% as.data.frame()

left_right_players <-  Analyzed_data %>% group_by(Analyzed_data$Preferred.Foot) %>% count()%>% as.data.frame()


Y <- na.omit(Y) 
Y_table = Y %>% as.data.frame()


set.seed(100)
Y_split = initial_split(Y_table, prop = 0.8)
Y_train = training(Y_split)
Y_test = testing(Y_split)


```

we first browse the summary of players' abilities whose dominant foot is left foot, which are in the appendix (end of this project).


Then we browse the summary of players' abilities whose dominant foot is right foot, which are also in the appendix(end of this project):

This tables shows how many left-footed players and right-footed players in the dataset.
```{r show left right foot difference, echo=FALSE}


kable(left_right_players, caption = 'Number of preferred foot', encoding = 'UTF-8')

```

We can see that in terms of almost all abilities, left-footed players and right-footed players are different (left-footed palyers's ability values are higher in most cases), as evidenced by both the median and mean. Of course, we have to mention one important thing, there are far fewer left-footed players than right-footed players. But based on this difference, I tried to use machine learning to predict the player's dominant foot by calculating various ability values of a player. 

Potential Significance: Since left-footed players have higher stats than right-footed players in terms of most ability, if we predict based on a player's stats that his dominant foot is the left foot but his dominant foot is actually the right foot, it means that under the same circumstances, He probably surpasses someone of the same ability but is right footed, in other words, at his own level, he is better in terms of preferred foot, otherwise the model would estimate he is right footed. This can be used as a form of self-encouragement.

I split the initial data set into train set and test set, and the splitting ratio is 0.8.

# Methods

The data we mainly use in this project include: 

*Dependent variable:* __perfoot_index__, preferred left foot is 0, preferred right foot is 1. 

*Independent variable:* __Age,  height_cm, weight_amount_2, Crossing, Finishing, HeadingAccuracy__ and other ability values.

I mainly use 3 methods, LASSO method (based on AIC and based on cross-validation), logistic model, and random forest model. First, I split dataset into training set and test set. Second, I use these methods and do regression. Third, I use the estimated regression model and test their accuracy based on test set. Finally, I create a ROC curve and f1 score for each model, judge which is the best model to complete my goal. 


### LASSO model

I first use `gamlr` package to do the regression, and I choose the coefficient based on the AIC measurement. In this part, I will show the plots of regression result, and the plot of AIC depends on different lambda.
```{r lasso based on AIC, include=FALSE}

scx = model.matrix(perfoot_index ~ .-1, data=Y_train) %>% scale()
# do -1 to drop intercept!

scy = Y_train$perfoot_index %>% as.integer() 

sclasso = gamlr(scx, as.factor(scy), family="binomial", nlambda = 200, alpha = 1)
plot(sclasso)

# AIC selected coef
# note: AICc = AIC with small-sample correction.  See ?AICc
#AICc(sclasso)  # the AIC values for all values of lambda
plot(sclasso$lambda, AICc(sclasso))
plot(log(sclasso$lambda), AICc(sclasso))

min_lambda <- log(sclasso$lambda[which.min(AICc(sclasso))])

scbeta = coef(sclasso) %>% round(3)
```


we can show min_lambda and how many coefficient is not equal to 0 under this LASSO model with AIC approximation, since we will use these info to do prediction for testing data set.
```{r show min lambda, echo=FALSE}
print('minimum lambda is:')
min_lambda

print('number of coefficient that is not equal to 0')
sum(scbeta!=0)

```


```{r Lasso with AIC prediction, include=FALSE}

lasso_predict = predict(sclasso, Y_test[2:38], lambda = min_lambda, type = 'response') ##### that is new data for prediction, cannot include the dependent variable in the prediction!!!!


AIClasso_predct_result = ifelse(lasso_predict > 0.5, 1, 0)
AIClasso_result_table = table(y = Y_test$perfoot_index, yhat = AIClasso_predct_result)
kable(AIClasso_result_table, caption = "LASSO result when choose min lambda")

```


Now I try LASSO regression without AIC approximation, but based on  cross validation. Then I plot the comparison plot between AICc and Cross Validation. In this case, I set `nfold=10`
```{r withou AIC, echo=FALSE}
# Now without the AIC approximation:
# cross validated lasso (`verb` just prints progress)
# this takes a little longer, but still so fast compared to stepwise
n = nrow(Y_train)
sccvl = cv.gamlr(scx, scy, nfold=10, family="binomial", verb=TRUE)

# plot the out-of-sample deviance as a function of log lambda
# Q: what are the bars associated with each dot? 
plot(sccvl, bty="n")

## CV min deviance selection
scb.min = coef(sccvl, select="min")
# log(sccvl$lambda.min)
# sum(scb.min!=0) # note: this is random!  because of the CV randomness

## CV 1se selection (the default)
scb.1se = coef(sccvl) %>% round(3)
# log(sccvl$lambda.1se)
# sum(scb.1se!=0) ## usually selects all zeros (just the intercept)

## comparing AICc and the CV error
# note that AIC is a pretty good estimate of out-of-sample deviance
# for values of lambda near the optimum
# outside that range: much worse  
plot(sccvl, bty="n",)
lines(log(sclasso$lambda),AICc(sclasso)/n, col="green", lwd=2)
legend("top", fill=c("blue","green"),
       legend=c("CV","AICc"), bty="n")
```
I use Lasso do some prediction, in this case I use the coefficient chose on 1 standard error through LASSO cross validation result. And I set `ifelse(lasso_predict > 0.5, 1, 0)`, and it shows the result like:  
```{r cv lasso, echo=FALSE}
pred_cvlasso = predict(sccvl, Y_test[2:38], lambda = scb.min, type="response")
pred_cvlasso_result = ifelse(pred_cvlasso > 0.5, 1, 0)
cvlasso_table = table(y = Y_test$perfoot_index, yhat = pred_cvlasso_result)

kable(cvlasso_table, caption = "LASSO based on cross validation")
```



## logistic regression


In this part, I try logistic model to estimate players' preferred feet. And it provides the result like (under `ifelse(log_prediction > 0.5, 1, 0)`): 
```{r   logit model distinguish prefer foot, echo=FALSE}
log_foot = glm(perfoot_index ~ .,data = Y_train, family = 'binomial')

log_prediction = predict(log_foot, Y_test)

log_predct_result = ifelse(log_prediction > 0.5, 1, 0)

result_table = table(y = Y_test$perfoot_index, yhat = log_predct_result)

log_coefficient = log_foot %>% coefficients()%>% round(3)

kable(result_table, caption = "logistic model prediction")
```



## stepwise function
I use the step wise function to  estimate players' preferred feet.In this case, I chose forward selection method. My initial regression is `null = glm(perfoot_index ~ 1, data=Y_train, family=binomial)`, and my final regression is `full = glm(perfoot_index ~ ., data=Y_train, family=binomial)`. It will do the estimation step by step. Below it gives part of prediction result, under `ifelse(stepwise_pred>0.5, 1, 0)`. 


```{r stepwise, include=FALSE}
full = glm(perfoot_index ~ ., data=Y_train, family=binomial)

## A forward stepwise procedure
# null model
null = glm(perfoot_index ~ 1, data=Y_train, family=binomial)

lm_step = step(full, 
               scope=~(.))

step_coef = coefficients(lm_step) %>% round(3)
# forward stepwise: it takes a long time!
# system.time(fwd <- step(null, scope=formula(full), dir="forward"))
stepwise_pred = predict(lm_step, Y_test)


```

```{r show the result, echo=FALSE}
stepwise_pred = predict(lm_step, Y_test)
stepwise_pred_result = ifelse(stepwise_pred>0.5, 1, 0)
result_table = table(y = Y_test$perfoot_index, yhat = stepwise_pred_result)
kable(result_table, caption = "step wise function prediction")
```

## Tree model

I use random forest tree model to do the regression. I first create a mannual tree model, but I don't know how to decide complexity parameter, so I just use random forest model, but I still keep the code in case readers want to check is there any difference between these two methods. It also provide some prediction result(under `ifelse(rf_pred > 0.5, 1, 0)`):
```{r tree model single and random forest, include=FALSE}

load.tree = rpart(perfoot_index ~ .,
                  data=Y_train, control = rpart.control(cp = 0.001))



# now a random forest
# notice: no tuning parameters!  just using the default
# downside: takes longer because we're fitting hundreds of trees (500 by default)
# the importance=TRUE flag tells randomForest to calculate variable importance metrics
load.forest = randomForest(perfoot_index ~ .,
                  data=Y_train, importance = TRUE)


# randomforest_coeff = coefficients(load.forest) %>% round(3) no coefficient in tree model
```

```{r tree model prediction, echo = FALSE}
rf_pred = predict(load.forest, Y_test)
rf_pred_result = ifelse(rf_pred > 0.5, 1, 0)
rf_pred_result_table = table(y = Y_test$perfoot_index, yhat = rf_pred_result)
kable(rf_pred_result_table, caption = "random forest tree model")
```

In this case, I plot the regression result and Variable Importance Plot, since it would be more straightforward to check the result.

```{r show result, echo=FALSE}
plot(load.forest)
vi = varImpPlot(load.forest, type=1)
vi
```


# result

I use two different methods __ROC curve and AUC value__ & __f1 Score__ to make comparison with various models, and both of them give me same answer, **step wise** model's performance is the best one. So we can use it to make prediction for a player's dominant foot.

## ROC curve
Since we cannot judge any model's accuracy based on single threshold in terms of the binomial variable (left or right), so I create a ROC curve and see their performance. In this case, I set a series of thresholds for the final binomial variable determination, which is a series of data `thresh_grid = seq(0.94, 0.45, by=-0.001)`. Since different models ROC curves are overlapping, so I just separate them into two plots in terms of two FPR intervals.
```{r ROC curves combination, echo=FALSE, message=FALSE, fig.align='center', warning=FALSE}



lasso_predict = predict(sclasso, Y_test[2:38], lambda = min_lambda, type = 'response')
pred_cvlasso = predict(sccvl, Y_test[2:38], type="response")
log_prediction = predict(log_foot, Y_test, type = "response")
stepwise_pred = predict(lm_step, Y_test, type = 'response')
tree_pred = predict(load.forest, Y_test, type = 'response')

thresh_grid = seq(0.94, 0.45, by=-0.0001)
roc_curve_spam = foreach(thresh = thresh_grid, .combine='rbind') %do% {
  yhat_lasso_predict = ifelse(lasso_predict >= thresh, 1, 0)
  yhat_pred_cvlasso = ifelse(pred_cvlasso >= thresh, 1, 0)
  yhat_log_prediction = ifelse(log_prediction >= thresh, 1, 0)
  yhat_stepwise_prediction = ifelse(stepwise_pred >= thresh, 1, 0)
  yhat_tree_prediction = ifelse(tree_pred >= thresh, 1, 0)
  
  # FPR, TPR for linear model
  confusion_out_lasso = table(y = Y_test$perfoot_index, yhat = yhat_lasso_predict)
  confusion_out_cvlasso = table(y = Y_test$perfoot_index, yhat = yhat_pred_cvlasso)
  confusion_out_log = table(y = Y_test$perfoot_index, yhat = yhat_log_prediction)
  confusion_out_stepwise = table(y = Y_test$perfoot_index, yhat = yhat_stepwise_prediction)
  confusion_out_tree = table(y = Y_test$perfoot_index, yhat = yhat_tree_prediction)
  
  # lasso output data frame
  out_lasso = data.frame(model = "lasso",
                       TPR = confusion_out_lasso[2,2]/sum(Y_test$perfoot_index==1),
                       FPR = confusion_out_lasso[1,2]/sum(Y_test$perfoot_index==0))
  
  # cvlasso output data frame
  out_cvlasso = data.frame(model = "lasso_cv",
                       TPR = confusion_out_cvlasso[2,2]/sum(Y_test$perfoot_index==1),
                       FPR = confusion_out_cvlasso[1,2]/sum(Y_test$perfoot_index==0))
  # logit model output data frame
   out_logit = data.frame(model = "logistic",
                       TPR = confusion_out_log[2,2]/sum(Y_test$perfoot_index==1),
                       FPR = confusion_out_log[1,2]/sum(Y_test$perfoot_index==0))
   # stepwise function output data frame
   out_stepwise = data.frame(model = 'stepwise function',
                       TPR = confusion_out_stepwise[2,2]/sum(Y_test$perfoot_index==1),
                       FPR = confusion_out_stepwise[1,2]/sum(Y_test$perfoot_index==0))
   
   # tree model output data frmae
   out_tree = data.frame(model = 'tree model',
                       TPR = confusion_out_tree[2,2]/sum(Y_test$perfoot_index==1),
                       FPR = confusion_out_tree[1,2]/sum(Y_test$perfoot_index==0))
   
  rbind(out_lasso, out_cvlasso, out_logit, out_stepwise, out_tree)
} %>% as.data.frame()
ggplot(roc_curve_spam) + 
  geom_line(aes(x=FPR, y=TPR, color=model)) + 
  labs(title="ROC curves: LASSO vs. LASSO_CV vs. logit model vs. randomforest model") +
  theme_bw(base_size = 10)

ggplot(roc_curve_spam) + 
  geom_line(aes(x=FPR, y=TPR, color=model)) + 
  labs(title="ROC curves in FPR equal to (0 - 0.5)") +
  xlim(0, 0.5) + 
  theme_bw(base_size = 10)

ggplot(roc_curve_spam) + 
  geom_line(aes(x=FPR, y=TPR, color=model)) + 
  labs(title="ROC curves in FPR equal to (0.5 - 1.0)") +
  xlim(0.5, 1) + 
  theme_bw(base_size = 10)



```


Over all, It shows almost ROC curves of __logistic model__, __stepwise model__ and __random forest model__ are overlapped for each threshold, but __stepwise__ function's performance is a little better since its curve is always above other models' curves. To make our final judgement, we introduce _AUC Score_ and __f1 score__ to do deeper analysis. 

## AUC Score

AUC represents the probability that a random positive example is positioned to the right of a random negative example, it stands for "Area under the ROC Curve.".
```{r calculate auc score, echo=FALSE}

auc_LASSO_AIC =auc(Y_test$perfoot_index,lasso_predict)
auc_LASSO_CV = auc(Y_test$perfoot_index, pred_cvlasso) 
auc_logit = auc(Y_test$perfoot_index, log_prediction)
auc_stepwise = auc(Y_test$perfoot_index, stepwise_pred)
auc_tree = auc(Y_test$perfoot_index, tree_pred)

modelname <- c("LASSO_AIC", "LASSO_CV", "Logistic", "stepwise", "randomforest")
AUC_value <- c(auc_LASSO_AIC, auc_LASSO_CV, auc_logit, auc_stepwise, auc_tree)

AUC_summary = data.frame(modelname, AUC_value)

kable(AUC_summary, caption = 'AUC value for different models')
```


According to the AUC values table, we cannot say that which models are best models to determine a player's dominant foot based on his various abilities, as their AUC score is very close. On the other hand, we can say that almost all models' performance are similar in terms of AUC score. 


## f1 score
I introduced f1 score for further comparison. The F1-score combines the precision and recall of a classifier into a single metric by taking their harmonic mean. It is primarily used to compare the performance of two classifiers. In general, higher a model's F1-score, better performance for this model.  
```{r f1_socre compare for different models, echo=FALSE}



# model name
modelname <- c("LASSO_AIC", "LASSO_CV", "Logistic", "stepwise", "randomforest")



lasso_aic = f1Score(Y_test$perfoot_index, lasso_predict)
lasso_cv = f1Score(Y_test$perfoot_index, pred_cvlasso)
logistic = f1Score(Y_test$perfoot_index, log_prediction)
stepwise = f1Score(Y_test$perfoot_index, stepwise_pred)
randomforest = f1Score(Y_test$perfoot_index, tree_pred )

f1SCore <- c(lasso_aic, lasso_cv, logistic, stepwise, randomforest)
f1Score_result = data.frame(modelname, f1SCore)

kable(f1Score_result, caption = 'f1 score summary for different models')

```
According to the result, we can see that step wise model and logistic model has largest f1 scores( much higher than LASSO models, but random forest model would also be a good choice, its F1-score always close to step wise and logistic models), which make sure step wise model has better performance than other models for our problem. (This conclusion is based on each split process, and sometimes the f1 score of the step wise model is higher than any one model)


## coefficient comparison

I also post each model's coefficients value below:
```{r show the coefficients for different models, echo=FALSE}
modelname <- c("Ability Name","LASSO_AIC", "LASSO_CV", "Logistic", "stepwise")

scbeta_table = as.data.frame(as.matrix(scbeta)) %>% rownames_to_column()
scb.1se_table = as.data.frame(as.matrix(scb.1se)) %>% rownames_to_column()
# https://stackoverflow.com/questions/12029177/sparse-matrix-to-a-data-frame-in-r

# https://stackoverflow.com/questions/7531868/how-to-rename-a-single-column-in-a-data-frame
log_coefficient_dataframe = log_coefficient%>% as.data.frame()%>% rownames_to_column()
colnames(log_coefficient_dataframe) <- c('rowname','C1')
step_coef_datarame = step_coef %>% as.data.frame()%>%rownames_to_column()
colnames(step_coef_datarame) <- c('rowname','C2')
#randomforest_coeff_dataframe = randomforest_coeff %>% as.data.frame()

coefficients <- merge(scbeta_table,scb.1se_table, by = 'rowname', all = TRUE)
coefficients <- merge(coefficients, log_coefficient_dataframe, by = 'rowname', all = TRUE)
coefficients <- merge(coefficients, step_coef_datarame, by = 'rowname', all = TRUE)

colnames(coefficients) <- modelname

kable(coefficients, caption = "different models coefficient summary")
```

From the table, we can see that there exists some difference for models' coefficients, since different model set different intercepts. Step wise model uses fewer variable and set a larger intercept. Unfortunately, we cannot get the coefficients from random forest model, since it is another kind of estimation.

On the other hand, we can see that the absolute value of coefficients for different model has some similarities. According to the Variable Importance Plot from the random forest model, we know that **Crossing**, **ShortPassing**, **ShotPower** are top 3 most important variable, according to the coefficient table, **Crossing**, **ShortPassing** 's coefficients absolute value are relative larger than other variables' in terms of these 4 models. 

# Conclusion
The results show that the **step wise** and **logistic** models can give the better prediction results, but it does not mean that we can directly use these model's coefficient values. For example, each time we split data, it create a different test set, which could make selected variables by step wise different, and their coefficients may also be different. In theory, I should do many groupings and then calculate the average AUC and F1score for each model to avoid the problem of chance, but my computer limits me to doing so.


Because we use many models, we can make confident inferences based on similarities in model results. A plausible speculation is that, FIFA game officials tend to give left-footed players higher stats on **Crossing**, **FKAccuracy**, **SlidingTackle**, **Curve**, **SprintSpeed**, because the coefficients of these variables in almost all models are negative and much smaller than the other negative coefficients. Note that in our model, 0 is a left-footed player and 1 is a right-footed player. In addition, right-footed players always have a better **ShortPassing**, **Volleys**, **Vision**, **Finishing** ability, since these abilities' coefficients are positive and larger than others.

Interestingly, we can see where the specific advantages of different footed players are based on the coefficient table, which can also eliminate our common misconceptions. For me, I always think left-footed players are better at dribbling because most defenders are right-footed, but FIFA game officials don't think so. It does make left-footed players have a higher dribbling ability, but it is far less advantageous than Crossing.

Another aspect that I would have liked to cover, but it is really difficult to deal with, is the correlation between the various abilities of the players, in other words, the endogeneity problem. I try to use intersection for different variables, but my computer can’t run such a large-scale calculation.

# Appendix
left-footed players summary:

```{r left preferred foot summary, echo=FALSE}
summary(left_foot)%>% head()

```

right-footed players summary:

```{r right perferred foot summary, echo=FALSE}
summary(right_foot)
```
