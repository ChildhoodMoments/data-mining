---
title: "HW1"
author: "Lizhao"
date: "2022/2/9"
output:
  md_document

---
Author: 

Jyun_Yu_Cheng

Li_Zhao_Du

Yi_Ji_Gao
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## question 1

```{r, include=FALSE, message = FALSE,warning=FALSE}
library(tidyverse)
library(lubridate)
library(dplyr)
library(rsample)  # for creating train/test splits
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(knitr)
ABIA <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/ABIA.csv')

```
calculate the average departure delay time over 2008 and make a plot to show that

```{r the plot of average departure delay time, include=FALSE}

AverageDayDelay <- ABIA %>% 
  select(Year, Month, DayofMonth, ArrDelay, DepDelay, ) %>%
  mutate(departureday = make_datetime(Year, Month, DayofMonth)) 

delay <-  AverageDayDelay %>%
  group_by(departureday) %>%
  summarise(mean_dep_delay = mean(DepDelay, na.rm = TRUE))

average_departure_delay_time_over_2008 <- ggplot(data = delay, aes(x = departureday, y = mean_dep_delay)) +
  geom_line()+
  labs(title = "average departure delay time over 2008 ")+
  labs(x = 'date', y = '(minutes)')
```

```{r plot average departure delay time over 2008}
plot(average_departure_delay_time_over_2008)
```

we can see the result that in most time, the delay time flactulate between 0 and 20 minutes, and around Oct, the delay time is relatively small, around 5 minutes.


next we try to calculate the average delay time base on average day, to reduce unnecessary trouble, we calculate the average delay time by hour
```{r average departure delay time over a day, include=FALSE}
AverageSpecificTimeDelay <- ABIA %>%
  select(Year, Month, DayofMonth, CRSDepTime, ArrDelay, DepDelay) %>%
  mutate(departureday = make_datetime(Year, Month, DayofMonth, CRSDepTime %/% 100, CRSDepTime %% 100)) %>%
  mutate(CRSdeparttime = format(departureday, format = "%H"))
  

delayoneday = AverageSpecificTimeDelay %>%
  group_by(CRSdeparttime) %>%
  summarise(mean_dep_oneday = mean(DepDelay, na.rm = TRUE))



delay_day <- ggplot(delayoneday) +
  geom_line(aes(x = CRSdeparttime, y = mean_dep_oneday, group =1)) +
  labs(title = "average departure delay time over one day(measured by hours) ")+
  labs(x = 'time', y = '(minutes)')


```

```{r delay day}
plot(delay_day)
```

we can find from 6am, the average delay time is increased in general, and due to schedule arrangement, there is no flight departure between 00 and 06

so we give a suggestion: try to catch earlier flight rather than later flight



We finally analyzed the relationship between delay time and airlines. First, we selected airlines with more than 50 flights, and then calculated their average delay time according to their Time scheduled departure time.

```{r delay affected by airline, include=FALSE }
answer1 <- ABIA %>% 
  select(Year, Month, DayofMonth, CRSDepTime, DepDelay, UniqueCarrier) %>%
  mutate(departureday = make_datetime(Year, Month, DayofMonth, CRSDepTime %/% 100, CRSDepTime %% 100)) %>%
  mutate(CRSdeparttime = format(departureday, format = "%H"))

delay_airline = answer1 %>%
  group_by(CRSdeparttime, UniqueCarrier) %>%
  summarise(count = n(),
            mean_dep_delay = mean(DepDelay, na.rm = TRUE))

delay_affected_airline <-  delay_airline %>%
  filter(count > 50)%>%
  ggplot(aes(x = CRSdeparttime, y = mean_dep_delay)) +
  geom_line(group = 1)+
  facet_wrap(~UniqueCarrier, nrow = 6)+
  theme( axis.text.x = element_text(size = 6,angle = 45,hjust = 1))+
  labs(title = "average departure delay time for different airlines ")

```

```{r delay time affected by airline}
plot(delay_affected_airline)

```

from that plot we can see that overall, US airline has the minimum delay time, but we lack their data in some months or some time intervals, maybe they doesn't arrange flight line during these conditions.


## question 2
```{r question 2, include=FALSE}
billboard <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/billboard.csv')

```
part_A  Make a table of the top 10 most popular songs since 1958, as measured by the total number of weeks that a song spent on the Billboard Top 100.

we get the result:
```{r part A, include=FALSE}
# part_A  Make a table of the top 10 most popular songs since 1958, 
#as measured by the total number of weeks that a song spent on the Billboard Top 100. 
the_top_10_most_popular_songs = billboard %>%
  group_by(performer, song)%>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10, part_A)

```


```{r the top 10 most popular songs since 1958 table }
(the_top_10_most_popular_songs)

```
from the table we can get top 10 most popular songs since 1958

partB Is the "musical diversity" of the Billboard Top 100 changing over time?
```{r part B first step, include=FALSE}
# part B Is the "musical diversity" of the Billboard Top 100 changing over time?
#excludes the years 1958 and 2021   useful: https://www.datasciencemadesimple.com/remove-duplicate-rows-r-using-dplyr-distinct-function/#:~:text=Cool%20Text%20Symbol-,Remove%20Duplicate%20rows%20in%20R%20using%20Dplyr%20%E2%80%93%20distinct%20()%20function,variable%20or%20with%20multiple%20variable.

part_b =billboard %>%
  filter(year != 1958 & year != 2021) 

```
we first excludes the years 1958 and 2021
then we can counts the number of times that a given song appears on the Top 100 in a given year

```{r second step, include=FALSE}
part_b_1 <- part_b %>%
  group_by(year,song)%>%
  summarise(count = n())
number_of_times_that_a_given_song <- part_b_1 %>% distinct(song, .keep_all = TRUE)

```

```{r result of second step}
number_of_times_that_a_given_song
```

then we count the number of unique songs that appeared on the Top 100 in each year, irrespective of how many times it had appeared.

```{r partB last step, include=FALSE}
part_b_3 <- number_of_times_that_a_given_song %>%
  group_by(year) %>%
  summarise(count = length(unique(song)))

# make a line graph to show muscial diversity over the years
unique_music <- ggplot(data = part_b_3)+
  geom_line(aes(x = year, y = count, group = 1))+
  labs(title = "musical diversity over the years ")


```

```{r show musical diversity over the years}
plot(unique_music)
```


partC: "ten-week hit" as a single song that appeared on the Billboard Top 100 for at least ten weeks. 
we first find performer and music satisfy ten-week hit, then filter people who have less than 30 songs
```{r part c satisfied music, include=FALSE}
part_c_1 <- billboard %>% 
  group_by(song, performer)%>%
  summarise(count = n())%>%
  arrange(desc(count)) %>%
  filter(count >= 10)

```

```{r filter unqualified musician, include=FALSE}
part_c_2 <- part_c_1 %>%
  group_by(performer) %>%
  summarise(count = n())%>%
  filter(count >= 30) %>%
  arrange(desc(count))
```

we can get the plot like: 
```{r ten-week hit, include=FALSE}
ten_week_hit_musicians <- ggplot(data = part_c_2, )+
  geom_col(aes(reorder(performer, count), count))+
  labs(title = "ten-week hit performer rank")+
  coord_flip()
  
```

```{r ten_week_hit_musicians list}
plot(ten_week_hit_musicians)
```



## question 3

partA the 95th percentile of heights for female competitors across all Athletics events 
```{r Q3 input data, include=FALSE}
olympics_top20 = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/olympics_top20.csv")

```


```{r 95th percentile, echo =FALSE, warning=FALSE, include=FALSE}
part_a_1 <- olympics_top20 %>%
  filter(sex == "F" & sport == "Athletics")%>%
  group_by(event) %>%
  summarise(ninety_fifth_percentile_height = quantile(height, 0.95))%>%
  arrange(desc(ninety_fifth_percentile_height))%>%
  head(20)
part_a_2 <-  olympics_top20 %>%
  filter(sex == "F" & sport == "Athletics")
```

so we get  95 percentile height of each event and overall separately:
```{r partA result}
quantile(part_a_2$height, 0.95)
part_a_1
```



The 95th percentile of heights for female competitors across all Athletics events is 183




Partb greatest variability in competitor's heights
```{r variability, echo =FALSE, include=FALSE}
competitors_hightest_heights_variablity <- olympics_top20 %>%
  filter(sex=="F") %>%
  group_by(event) %>%
  summarize(height_var=sd(height)) %>%
  arrange(desc(height_var)) %>%
  head(1)
```

```{r show the result}
competitors_hightest_heights_variablity
```


Rowing Women's Coxed Fours had the greatest variability in competitor's heights across the entire history of the Olympics, as measured by the standard deviation

partc average age of Olympic swimmers changed over time
```{r Olympic swimmers average age plot , echo =FALSE, include=FALSE}
 average_swimmers_age_plot_based_on_gender <- olympics_top20 %>%
  filter(sport=="Swimming") %>%
  group_by(sex,year) %>%
  summarize(age_avg=mean(age)) %>%
  ggplot(aes(x=year,y=age_avg,color=sex))+
  geom_line()+
  geom_point(size = 4)

plot(average_swimmers_age_plot_based_on_gender)
```


according to data frame, we can also see this trend
```{r, include=FALSE}
average_dataframe <- olympics_top20 %>%
  filter(sport=="Swimming") %>%
  group_by(year,sex) %>%
  summarize(age_avg=mean(age))%>%
  arrange(year, )
```

```{r}
average_dataframe
```



```{r show average swimmers age change}
plot(average_swimmers_age_plot_based_on_gender)
```

The average age of Olympic swimmers increased over time.
The trend look different for male swimmers relative to female swimmers


## question 4 
```{r, echo =FALSE}

sclass <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/sclass.csv")
```

###1 Filter 350 & 65 AMG
```{r, echo =FALSE}
def_350 = sclass%>%
  filter(trim == "350")

def_65 = sclass%>%
  filter(trim == "65 AMG")
```

###2 spilt to test& training set
```{r, echo =FALSE}
def_350_spilt = initial_split(def_350 ,prop=0.8)
def_350_train = training(def_350_spilt)
def_350_test = testing(def_350_spilt)

def_65_spilt = initial_split(def_65 ,prop=0.8)
def_65_train = training(def_65_spilt)
def_65_test = testing(def_65_spilt)
```

###3 run k nearest neighbors  RMSEs
```{r, echo =FALSE}
def_350 = sclass%>%
  filter(trim == "350")

def_65 = sclass%>%
  filter(trim == "65 AMG")

def_350_spilt = initial_split(def_350 ,prop=0.8)
def_350_train = training(def_350_spilt)
def_350_test = testing(def_350_spilt)

def_65_spilt = initial_split(def_65 ,prop=0.8)
def_65_train = training(def_65_spilt)
def_65_test = testing(def_65_spilt)




k_folds = 5
k_grid = rep(1:100)



def_350 = def_350 %>%
  mutate(fold_id = rep(1:k_folds, length=nrow(def_350)) %>% sample)


def_65 = def_65 %>%
  mutate(fold_id = rep(1:k_folds, length=nrow(def_65)) %>% sample)


def_350_folds = crossv_kfold(def_350, k=k_folds)
def_65_folds = crossv_kfold(def_65, k=k_folds)

cv_grid_350 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(def_350_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, def_350_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(k_folds))
} %>% as.data.frame

cv_grid_65 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(def_65_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, def_65_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(k_folds))
} %>% as.data.frame

head(cv_grid_350)
head(cv_grid_65)


```

###4 the relationship for RMSE and k，can find optimal k（line or point）
```{r, echo =FALSE}
ggplot(cv_grid_350) + 
  geom_point(aes(x= k, y= err),size = 1) + 
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err))+
  geom_line(aes(x= k, y= err),size = 0.8)

ggplot(cv_grid_65) + 
  geom_point(aes(x= k, y= err), size =1) + 
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err))+
  geom_line(aes(x= k, y= err),size = 0.8)
```
Then we try to find the optimal k for each trim

```{r optimal k, include=FALSE}
k_min_rmse_350 = cv_grid_350 %>%
  slice_min(err) %>%
  pull(k)

knn350 = knnreg(price ~ mileage, data=def_350_train, k=k_min_rmse_350)
RMSE_k_min_rmse_350 = rmse(knn350, def_350_train)


k_min_rmse_65 = cv_grid_65 %>%
  slice_min(err) %>%
  pull(k)
knn65 = knnreg(price ~ mileage, data=def_65_train, k=k_min_rmse_350)
RMSE_k_min_rmse_65 = rmse(knn65, def_65_train)

```


so the optimal k for trim 350  and its RMSE is:
```{r}
k_min_rmse_350
RMSE_k_min_rmse_350
```
so the optimal k for trim 65_AUG and its RMSE is:
```{r}
k_min_rmse_65
RMSE_k_min_rmse_65
```


then we fit the model to the training set and make predictions on your test set
```{r, echo =FALSE}


# 350 

knn35_predict = knnreg(price ~ mileage, data=def_350_train, k=k_min_rmse_350)
RMSE_350_testset = rmse(knn35_predict, def_350_test)

def_350_test = def_350_test %>%
  mutate(price_350_pred = predict(knn35_predict, def_350_test))

p_test_350 = ggplot(data = def_350_test) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) +
  geom_line(aes(x = mileage , y = price_350_pred), color='blue', size=1.5)+ ggtitle("350 test result")



```
so  the out-of-sample root mean-squared error (RMSE)  for trim 350 is and its fitting graph is:
```{r}
RMSE_350_testset
```

```{r}
p_test_350
```


```{r}
#65 AMG

knn65_predict = knnreg(price ~ mileage, data=def_65_train, k=k_min_rmse_65)
RMSE_65_testset = rmse(knn65_predict, def_65_test)

def_65_test = def_65_test %>%
  mutate(price_65_pred = predict(knn65_predict, def_65_test))

p_test_65 = ggplot(data = def_65_test) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) +
  geom_line(aes(x = mileage , y = price_65_pred), color='red', size=1.5)+ggtitle(" 65 AMG, k=9")



```
so  the out-of-sample root mean-squared error (RMSE)  for trim 350 is and its fitting graph is:

```{r}
RMSE_65_testset
```

```{r}
p_test_65
```

350 trim has a bigger optimal value of k.

Because 65 AMG trim has smaller data size compare to 350 trim level. fewer observations.On the other hand, smaller estimation variance generally requires a less complex model, so we need set k smaller to contain various noises data.And when sample size is smaller, our setting k progress would be affected. 




